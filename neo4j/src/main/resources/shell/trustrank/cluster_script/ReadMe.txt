#client端需要解决的问题

1、删除旧数据 done
	保留的时间，最好可配置


2、跑spark程序
	1.1、判断数仓的数据是否生成
	1.2、判断跑的数据是否成功
	1.3、执行spark代码
		1、执行的日志
		2、执行的时间
	1.4、判断spark执行的是否成功
		1、根据APP_ID判断任务是否执行成功(/每隔1分钟check一次/)
			***yarn 相关小命令 slave32@tinyv:~/bin/***
		#	yarnstatus application_1509192804284_41524 2>/dev/null
			1、如果失败了
				1、记录失败
				2、发送邮件
				3、拉取集群日志
				4、中断之后的任务
				5、return 一个错代码（用于接下来的本地代码停止） 人工介入
				#6、touch一个成功或者失败的k文件 scp到neo4j机器上用于交互的逻辑判断
			2、如果成功了
				1、记录成功
				2、拉取集群日志
				3、继续之后的任务
		2、判断生成文件的大小，生成时间，可以记录日志


3、拉取spark任务生成的文件
	2.1、拉取生成的数据文件
		1、判断维度
			1、拉取命令的返回值
			2、数据文件的大小
		2、根据维度进行接下来操作
			1、如果成功
				1、记录拉取成功时间，拉取耗时，拉取文件的大小
			2、如果失败
				1、记录失败日志
				2、退出接下来的程序，发送错误代码k文件到远程停止远程等待

4、打包文件成tgz
	3.1、打包日志
	3.2、打包完成之后，生成 md5 ,记录到日志中（还是记录到k文件中，，，待定）

5、远程拷贝spark生成的文件到远程
	5.1、拷贝判断是否执行成功
6、文件备份15天其余日志及数据文件删除
